{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import time \n",
    "import gc \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, BatchNormalization, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import CuDNNGRU\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "import threading\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from contextlib import closing\n",
    "cores = 4\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.optimizers import RMSprop, Adam, Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import constraints\n",
    "from keras import regularizers\n",
    "from keras.engine import InputSpec, Layer\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 2.0.6\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.squeeze(K.dot(x, K.expand_dims(self.W)), axis=-1)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "    \n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_data(X, y, batch_size=64):\n",
    "    # Берём  индексы\n",
    "    # Перемешиваем\n",
    "    # Семплируем по размеру батча\n",
    "    idx = np.arange(X.shape[0])\n",
    "    random.shuffle(idx)\n",
    "    steps, last_batch_size = divmod(idx.shape[0]/batch_size)\n",
    "    if last_batch_size:\n",
    "        pass\n",
    "    for i in range(steps):\n",
    "        {\n",
    "            'x': X[batch*i:batch*(i+1)],\n",
    "            'y': y[batch*i:batch*(i+1)],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 127\n",
    "FEATURE_COUNT = 40\n",
    "EMBEDDING_MATRIX = np.random.uniform(size=[ FEATURE_COUNT, EMBEDDING_DIM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    series_id = Input(shape=[FEATURE_COUNT], name=\"series_id\")\n",
    "    series_values = Input(shape=[FEATURE_COUNT], name=\"series_values\")\n",
    "    series_values_reshape = Reshape([FEATURE_COUNT, 1])(series_values)\n",
    "    emb_series_id = Embedding(\n",
    "        FEATURE_COUNT,\n",
    "        EMBEDDING_DIM,\n",
    "        weights = [EMBEDDING_MATRIX], # Случайные? # Какое-нибудь разложение\n",
    "        trainable = True\n",
    "    )(series_id)\n",
    "    emb_series_id = SpatialDropout1D(0.1)(emb_series_id)\n",
    "    \n",
    "    \n",
    "    series = concatenate([\n",
    "        emb_series_id,\n",
    "        series_values_reshape \n",
    "    ])\n",
    "\n",
    "    rnn_out = CuDNNGRU(64, return_sequences=True) (series)\n",
    "    \n",
    "    attention_rnn_weighted_average = AttentionWeightedAverage()(rnn_out)    \n",
    "    # Здесь предварительно можно подумать о свёртках\n",
    "    mean_rnn = GlobalAveragePooling1D()(rnn_out)\n",
    "    max_rnn = GlobalMaxPooling1D()(rnn_out)\n",
    "    attention_rnn = Attention()(rnn_out)\n",
    "    \n",
    "    \n",
    "    main_l = concatenate([\n",
    "        mean_rnn,\n",
    "        max_rnn,\n",
    "        attention_rnn,\n",
    "        attention_rnn_weighted_average,\n",
    "    ])\n",
    "    \n",
    "    main_l = BatchNormalization()(main_l)\n",
    "    main_l = Dropout(0.5)(Dense(128, activation='relu') (main_l))\n",
    "    main_l = BatchNormalization()(main_l)\n",
    "    output = Dense(1,activation=None) (main_l)\n",
    "    \n",
    "    model = Model([series_id, series_values], output)\n",
    "#     model.compile(optimizer = 'adam',\n",
    "#                   loss= root_mean_squared_error,\n",
    "#                   metrics = [root_mean_squared_error])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "series_id (InputLayer)          (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 40, 127)      5080        series_id[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "series_values (InputLayer)      (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_3 (SpatialDro (None, 40, 127)      0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 40, 1)        0           series_values[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 40, 128)      0           spatial_dropout1d_3[0][0]        \n",
      "                                                                 reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)          (None, 40, 64)       37248       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           cu_dnngru_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 64)           0           cu_dnngru_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 64)           104         cu_dnngru_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_weighted_average_2 (A (None, 64)           64          cu_dnngru_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 attention_weighted_average_2[0][0\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          32896       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            129         batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 77,057\n",
      "Trainable params: 76,289\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(model_file, monitor='val_rmse', verbose=1, mode='min',\n",
    "                                   save_best_only=True, save_weights_only=False, period=1)\n",
    "\n",
    "clr = CyclicLR(base_lr=0.001, max_lr=0.01, step_size=2*math.ceil(y[trn_inx].shape[0]), mode='triangular2')\n",
    "early_stop = EarlyStopping(monitor='val_rmse', min_delta=0, patience=6, verbose=1, mode='min')\n",
    "# mse_eval = MSEEvaluationSeq(val_seq, y[val_inx], 'val')   \n",
    "\n",
    "\n",
    "# Training\n",
    "opt=optimizers.Nadam()\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy') # \n",
    "\n",
    "model.fit_generator(\n",
    "    generator=trn_seq, \n",
    "    steps_per_epoch=len(trn_seq), #?\n",
    "    initial_epoch=0,\n",
    "    epochs=epochs, shuffle=False, verbose=2,\n",
    "    callbacks=[mse_eval, model_checkpoint, clr],\n",
    "    use_multiprocessing=False, workers=1, max_queue_size=4*cpu_cores)\n",
    "\n",
    "# Predicting\n",
    "print(\"\\nPredicting fold {}\".format(ifold))\n",
    "del model, trn_seq, mse_eval, clr, early_stop, model_checkpoint\n",
    "model = keras.models.load_model(\n",
    "    model_file, \n",
    "    compile=True, \n",
    "    custom_objects={\n",
    "        'Attention':Attention, \n",
    "        'AttentionWeightedAverage':AttentionWeightedAverage}\n",
    ")\n",
    "pred[val_inx] = np.clip(model.predict_generator(val_seq, steps=len(val_seq), \n",
    "                                                use_multiprocessing=False, workers=1, \n",
    "                                                max_queue_size=4*cpu_cores).ravel(), 0.0, 1.0)\n",
    "del val_seq\n",
    "gc.collect()\n",
    "\n",
    "mse = metrics.mean_squared_error(y[val_inx], pred[val_inx])\n",
    "rmse = np.sqrt(mse)\n",
    "fold_mse.append(mse)\n",
    "fold_rmse.append(rmse)\n",
    "print(\"fold: {}, mse: {}, rmse: {}\".format(ifold, mse, rmse))\n",
    "print()\n",
    "\n",
    "test_pred += np.clip(model.predict(split_inputs(test_cat_features)+\\\n",
    "                                   [test_num_features, \n",
    "                                    test_te,\n",
    "                                    test_user_features, \n",
    "                                    test_image_features, \n",
    "                                    test_title_sequences[0],\n",
    "                                    test_descs_sequences[0], \n",
    "                                    test_fasttext_features,\n",
    "                                    test_char_title,\n",
    "                                    test_char_descs\n",
    "                                   ], \n",
    "                                   batch_size=INF_BATCH_SIZE, verbose=0).ravel(), 0.0, 1.0)/n_folds\n",
    "    ifold += 1\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
